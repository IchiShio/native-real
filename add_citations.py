#!/usr/bin/env python3
"""
å¼•ç”¨ãƒªãƒ³ã‚¯è‡ªå‹•ä»˜ä¸ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

data/citation_db.json ã®æ¤œè¨¼æ¸ˆã¿URLã‚’ä½¿ã„ã€articles/ ä»¥ä¸‹ã®HTMLã«
å¼•ç”¨ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•ã§ä»˜ä¸ã™ã‚‹ã€‚

å‹•ä½œä»•æ§˜:
  - <a>, <style>, <script> ã‚¿ã‚°ã®å†…éƒ¨ãƒ†ã‚­ã‚¹ãƒˆã¯å¤‰æ›´ã—ãªã„
  - å„å¼•ç”¨ã‚½ãƒ¼ã‚¹ã¯è¨˜äº‹å†…ã§æœ€åˆã®ãƒãƒƒãƒ1ç®‡æ‰€ã®ã¿ã«ãƒªãƒ³ã‚¯ã‚’ä»˜ä¸ï¼ˆéå‰°ãƒªãƒ³ã‚¯é˜²æ­¢ï¼‰
  - ã™ã§ã« <a href> ã§ãƒ©ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹ç®‡æ‰€ã¯ã‚¹ã‚­ãƒƒãƒ—

ä½¿ã„æ–¹:
  python3 add_citations.py                # å…¨è¨˜äº‹ã«é©ç”¨
  python3 add_citations.py --dry-run      # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãªã—ï¼‰
  python3 add_citations.py articles/english-career-salary-impact  # ç‰¹å®šè¨˜äº‹ã®ã¿
  python3 add_citations.py --verify       # citation_db.json ã®å…¨URLã‚’ç–é€šç¢ºèª
"""

from __future__ import annotations

import argparse
import json
import re
import sys
import urllib.request
from pathlib import Path

SITE_DIR = Path(__file__).parent
CITATION_DB_PATH = SITE_DIR / "data" / "citation_db.json"
ARTICLES_DIR = SITE_DIR / "articles"

# HTMLã‚¿ã‚°ã®æ­£è¦è¡¨ç¾ï¼ˆscript/style ãƒ–ãƒ­ãƒƒã‚¯ã‚’å«ã‚€ï¼‰
_TAG_RE = re.compile(r"(<[^>]+>)", re.DOTALL)
_OPEN_A_RE = re.compile(r"^<a\b", re.IGNORECASE)
_CLOSE_A_RE = re.compile(r"^</a\s*>$", re.IGNORECASE)
_OPEN_STYLE_RE = re.compile(r"^<style\b", re.IGNORECASE)
_CLOSE_STYLE_RE = re.compile(r"^</style\s*>$", re.IGNORECASE)
_OPEN_SCRIPT_RE = re.compile(r"^<script\b", re.IGNORECASE)
_CLOSE_SCRIPT_RE = re.compile(r"^</script\s*>$", re.IGNORECASE)


def load_citation_db() -> list[dict]:
    """citation_db.json ã‚’èª­ã¿è¾¼ã¿ã€é©ç”¨å¯èƒ½ãªã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    with open(CITATION_DB_PATH, encoding="utf-8") as f:
        raw = json.load(f)
    entries = []
    for key, val in raw.items():
        if key.startswith("_"):
            continue  # READMEç­‰ã®ãƒ¡ã‚¿ã‚­ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—
        entries.append({"id": key, **val})
    return entries


def apply_citations_to_text(text: str, entries: list[dict], applied: set[str]) -> tuple[str, list[str]]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰ã«å¼•ç”¨ãƒªãƒ³ã‚¯ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    applied: ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§æ—¢ã«ãƒªãƒ³ã‚¯ä»˜ä¸æ¸ˆã¿ã®ã‚¨ãƒ³ãƒˆãƒªIDé›†åˆï¼ˆæ›´æ–°ã•ã‚Œã‚‹ï¼‰
    """
    changes = []
    for entry in entries:
        if entry["id"] in applied:
            continue  # æ—¢ã«ã“ã®è¨˜äº‹ã§ãƒªãƒ³ã‚¯æ¸ˆã¿ â†’ ã‚¹ã‚­ãƒƒãƒ—
        url = entry["url"]
        label = entry["label"]
        for kw_pattern in entry["keywords"]:
            try:
                m = re.search(kw_pattern, text)
                if m:
                    matched_text = m.group(0)
                    link = f'<a href="{url}" target="_blank" rel="noopener">{matched_text}</a>'
                    text = text[:m.start()] + link + text[m.end():]
                    applied.add(entry["id"])
                    changes.append(f"  [{entry['id']}] ã€Œ{matched_text[:40]}ã€â†’ {url}")
                    break  # ã“ã®ã‚¨ãƒ³ãƒˆãƒªã¯1ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã§çµ‚äº†
            except re.error as e:
                print(f"  âš  æ­£è¦è¡¨ç¾ã‚¨ãƒ©ãƒ¼ [{entry['id']}] '{kw_pattern}': {e}", file=sys.stderr)
    return text, changes


def apply_citations_to_html(html: str, entries: list[dict]) -> tuple[str, list[str]]:
    """
    HTMLå…¨ä½“ã«å¼•ç”¨ãƒªãƒ³ã‚¯ã‚’ä»˜ä¸ã™ã‚‹ã€‚
    <a>, <style>, <script> ã®å†…éƒ¨ãƒ†ã‚­ã‚¹ãƒˆã¯å¤‰æ›´ã—ãªã„ã€‚
    """
    parts = _TAG_RE.split(html)
    depth_a = 0
    depth_style = 0
    depth_script = 0
    applied: set[str] = set()
    result = []
    all_changes = []

    for part in parts:
        if _TAG_RE.match(part):
            # ã‚¿ã‚°éƒ¨åˆ†ï¼šãƒã‚¹ãƒˆæ·±åº¦ã‚’æ›´æ–°
            if _OPEN_A_RE.match(part):
                depth_a += 1
            elif _CLOSE_A_RE.match(part):
                depth_a = max(0, depth_a - 1)
            elif _OPEN_STYLE_RE.match(part):
                depth_style += 1
            elif _CLOSE_STYLE_RE.match(part):
                depth_style = max(0, depth_style - 1)
            elif _OPEN_SCRIPT_RE.match(part):
                depth_script += 1
            elif _CLOSE_SCRIPT_RE.match(part):
                depth_script = max(0, depth_script - 1)
            result.append(part)
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ¼ãƒ‰
            if depth_a > 0 or depth_style > 0 or depth_script > 0:
                result.append(part)
            else:
                new_part, changes = apply_citations_to_text(part, entries, applied)
                result.append(new_part)
                all_changes.extend(changes)

    return "".join(result), all_changes


def process_file(html_path: Path, entries: list[dict], dry_run: bool) -> int:
    """
    1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã€‚å¤‰æ›´ä»¶æ•°ã‚’è¿”ã™ã€‚
    """
    original = html_path.read_text(encoding="utf-8")
    new_html, changes = apply_citations_to_html(original, entries)

    if not changes:
        return 0

    article_name = html_path.parent.name
    print(f"\nğŸ“„ {article_name}")
    for c in changes:
        print(c)

    if not dry_run and new_html != original:
        html_path.write_text(new_html, encoding="utf-8")

    return len(changes)


def verify_urls(entries: list[dict]) -> None:
    """citation_db.json ã®å…¨URLã‚’ç–é€šç¢ºèªã™ã‚‹"""
    print("ğŸ” citation_db.json ã®å…¨URLã‚’ç–é€šç¢ºèªä¸­...\n")
    ok = 0
    ng = 0
    for entry in entries:
        url = entry["url"]
        try:
            req = urllib.request.Request(url, headers={"User-Agent": "Mozilla/5.0"})
            with urllib.request.urlopen(req, timeout=10) as res:
                status = res.status
            if status == 200:
                print(f"  âœ… [{entry['id']}] {url}")
                ok += 1
            else:
                print(f"  âš  [{entry['id']}] HTTP {status}: {url}")
                ng += 1
        except Exception as e:
            print(f"  âŒ [{entry['id']}] ERROR {e}: {url}")
            ng += 1
    print(f"\nçµæœ: {ok}ä»¶OK / {ng}ä»¶NG")
    if ng > 0:
        print("âŒ NGã®URLã¯ citation_db.json ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
    else:
        print("âœ… å…¨URLç–é€šç¢ºèªæ¸ˆã¿")


def main() -> None:
    parser = argparse.ArgumentParser(description="è¨˜äº‹ã«å¼•ç”¨ãƒªãƒ³ã‚¯ã‚’è‡ªå‹•ä»˜ä¸ã™ã‚‹")
    parser.add_argument("target", nargs="?", help="ç‰¹å®šè¨˜äº‹ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯å…¨è¨˜äº‹ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="å¤‰æ›´å†…å®¹ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°ãªã—ï¼‰")
    parser.add_argument("--verify", action="store_true", help="citation_db.json ã®å…¨URLã‚’ç–é€šç¢ºèªã™ã‚‹")
    args = parser.parse_args()

    entries = load_citation_db()

    if args.verify:
        verify_urls(entries)
        return

    # å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
    if args.target:
        targets = [Path(args.target)]
    else:
        targets = sorted(ARTICLES_DIR.iterdir())

    if args.dry_run:
        print("ğŸ” ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¯æ›´æ–°ã—ã¾ã›ã‚“ï¼‰\n")

    total_files = 0
    total_changes = 0

    for target in targets:
        html = target / "index.html" if target.is_dir() else target
        if not html.exists() or html.suffix != ".html":
            continue
        if html.parent.name == "articles" and html.name == "index.html":
            continue  # è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ã¯ã‚¹ã‚­ãƒƒãƒ—

        n = process_file(html, entries, dry_run=args.dry_run)
        if n > 0:
            total_files += 1
            total_changes += n

    print(f"\n{'=' * 60}")
    if args.dry_run:
        print(f"[ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³] {total_files} è¨˜äº‹ / {total_changes} ç®‡æ‰€ã«ãƒªãƒ³ã‚¯ä»˜ä¸äºˆå®š")
        print("å®Ÿéš›ã«é©ç”¨ã™ã‚‹ã«ã¯ --dry-run ã‚’å¤–ã—ã¦å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    else:
        print(f"âœ… å®Œäº†: {total_files} è¨˜äº‹ / {total_changes} ç®‡æ‰€ã«ãƒªãƒ³ã‚¯ä»˜ä¸ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()
