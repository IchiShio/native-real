#!/usr/bin/env python3
"""
publishå‰çµ±è¨ˆãƒã‚§ãƒƒã‚«ãƒ¼

eikaiwa-hikaku ã® articles/ ä»¥ä¸‹ã®HTMLã‚’èµ°æŸ»ã—ã€
ç¢ºèªãŒå¿…è¦ãªçµ±è¨ˆãƒ»å¼•ç”¨è¡¨ç¾ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã€‚
ã¾ãŸã€citation_db.json ã«ç™»éŒ²æ¸ˆã¿ã®ã‚½ãƒ¼ã‚¹ãŒå¼•ç”¨ãƒªãƒ³ã‚¯ãªã—ã§
ä½¿ã‚ã‚Œã¦ã„ã‚‹ç®‡æ‰€ã‚‚æ¤œå‡ºã™ã‚‹ï¼ˆadd_citations.py ã‚’å®Ÿè¡Œã™ã‚Œã°è§£æ¶ˆï¼‰ã€‚

ä½¿ã„æ–¹:
  # å…¨è¨˜äº‹ã‚’æ¤œæŸ»
  python3 check_stats.py

  # ç‰¹å®šã®è¨˜äº‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®š
  python3 check_stats.py articles/english-career-salary-impact
"""

import json
import re
import sys
from pathlib import Path

# native-real ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ«ãƒ¼ãƒˆï¼ˆã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªèº«ã®å ´æ‰€ï¼‰
SITE_DIR = Path(__file__).parent
CITATION_DB_PATH = SITE_DIR / "data" / "citation_db.json"

# è¦ç¢ºèªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ­£è¦è¡¨ç¾ï¼‰
PATTERNS = [
    # å¤–éƒ¨èª¿æŸ»ã®æ•°å€¤å¼•ç”¨
    (r"([\wãƒ»]+(?:èª¿æŸ»|ç ”ç©¶æ‰€|æ©Ÿé–¢|å”ä¼š|ç·ç ”)).*?(\d+[%ï¼…])", "å¤–éƒ¨çµ±è¨ˆå¼•ç”¨"),
    (r"ã«ã‚ˆã‚‹ã¨.*?(\d+[%ï¼…])",                                  "ã€Œã«ã‚ˆã‚‹ã¨+%ã€"),
    (r"ã«ã‚ˆã‚Œã°.*?(\d+[%ï¼…])",                                  "ã€Œã«ã‚ˆã‚Œã°+%ã€"),
    (r"ã«æ²è¼‰ã•ã‚ŒãŸç ”ç©¶",                                        "ã‚¸ãƒ£ãƒ¼ãƒŠãƒ«å¼•ç”¨"),
    (r"ã®ç ”ç©¶(?:ã§ã¯|ã«ã‚ˆã‚‹ã¨|ã«ã‚ˆã‚Œã°)",                         "ç ”ç©¶è€…å¼•ç”¨"),
    # FSI ã®èª¤ç”¨
    (r"FSI|Foreign Service Institute",                          "FSIå¼•ç”¨ï¼ˆè¦ç¢ºèªï¼‰"),
    # æé€ ã•ã‚Œã‚„ã™ã„çµ„ç¹”
    (r"MMDç ”ç©¶æ‰€|ã‚¨ãƒ³ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³|èˆ¹äº•ç·ç ”",                        "è¦æ³¨æ„çµ„ç¹”å"),
    # æ ¹æ‹ ãªãå…·ä½“æ•°å€¤
    (r"(?:ç´„|å¹³å‡)\d+[%ï¼…](?:å‘ä¸Š|æ”¹å–„|å¢—åŠ |æ¸›å°‘)",              "åŠ¹æœã®å…·ä½“%"),
    (r"\d+å€(?:ã¨|ã«|ãŒ)(?:å ±å‘Š|ç¤º|ç¢ºèª)",                       "ã€‡å€ã®å¼•ç”¨"),
]

# è¨±å®¹ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã“ã‚Œã¯ç„¡è¦–ï¼‰
ALLOWLIST_PATTERNS = [
    r"ã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ",
    r"width|height|border|padding|margin|font|color|px|em|rem",
    r"display|position|flex|grid|background|opacity|z-index",
    r"border-radius|line-height|pointer|overflow",
]


def is_allowlisted(line: str) -> bool:
    return any(re.search(p, line) for p in ALLOWLIST_PATTERNS)


def check_file(html_path: Path) -> list[tuple[int, str, str]]:
    """1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œæŸ»ã—ã¦ (è¡Œç•ªå·, ãƒãƒƒãƒç†ç”±, è¡Œãƒ†ã‚­ã‚¹ãƒˆ) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    hits = []
    text = html_path.read_text(encoding="utf-8")
    for lineno, line in enumerate(text.splitlines(), 1):
        if is_allowlisted(line):
            continue
        for pattern, label in PATTERNS:
            if re.search(pattern, line):
                snippet = line.strip()[:120]
                hits.append((lineno, label, snippet))
                break  # 1è¡Œã«ã¤ã1ä»¶ã®ã¿å ±å‘Š
    return hits


def load_citation_keywords() -> list[tuple[str, str, str]]:
    """citation_db.json ã‹ã‚‰ (entry_id, label, pattern) ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™"""
    try:
        with open(CITATION_DB_PATH, encoding="utf-8") as f:
            db = json.load(f)
    except FileNotFoundError:
        return []
    result = []
    for key, val in db.items():
        if key.startswith("_"):
            continue
        for kw in val.get("keywords", []):
            result.append((key, val["label"], kw))
    return result


def check_citation_links(
    html_path: Path, citation_entries: list[tuple[str, str, str]]
) -> list[tuple[str, str]]:
    """citation_db ã«ç™»éŒ²æ¸ˆã¿ã®ã‚½ãƒ¼ã‚¹ãŒ <a> ãƒªãƒ³ã‚¯ãªã—ã§ä½¿ã‚ã‚Œã¦ã„ã‚‹ç®‡æ‰€ã‚’æ¤œå‡ºã™ã‚‹"""
    text = html_path.read_text(encoding="utf-8")
    flagged: set[str] = set()
    hits = []
    for entry_id, label, kw_pattern in citation_entries:
        if entry_id in flagged:
            continue
        try:
            m = re.search(kw_pattern, text)
            if not m:
                continue
            # ãƒãƒƒãƒä½ç½®ãŒæ—¢ã« <a> ã‚¿ã‚°å†…ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“åˆ¤å®šï¼‰
            before = text[: m.start()]
            last_open_a = before.rfind("<a ")
            last_close_a = before.rfind("</a>")
            if last_open_a > last_close_a:
                continue  # æ—¢ã«ãƒªãƒ³ã‚¯æ¸ˆã¿
            flagged.add(entry_id)
            hits.append((label, m.group(0)[:60]))
        except re.error:
            pass
    return hits


def main():
    # æ¤œæŸ»å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ±ºå®š
    if len(sys.argv) > 1:
        targets = [Path(sys.argv[1])]
    else:
        articles_dir = SITE_DIR / "articles"
        if not articles_dir.exists():
            print(f"Error: {articles_dir} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            sys.exit(1)
        targets = sorted(articles_dir.iterdir())

    # HTMLãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆå†åˆ©ç”¨ã™ã‚‹ãŸã‚ï¼‰
    html_files: list[tuple[str, Path]] = []
    for target in targets:
        html = target / "index.html" if target.is_dir() else target
        if html.exists() and html.suffix == ".html":
            name = target.name if target.is_dir() else target.stem
            html_files.append((name, html))

    # â”€â”€ çµ±è¨ˆãƒã‚§ãƒƒã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    total_hits = 0
    flagged_articles = []

    for name, html in html_files:
        hits = check_file(html)
        if hits:
            flagged_articles.append((name, hits))
            total_hits += len(hits)

    if not flagged_articles:
        print("âœ… è¦ç¢ºèªã®çµ±è¨ˆãƒ»å¼•ç”¨ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        print(f"âš ï¸  {len(flagged_articles)} è¨˜äº‹ã§ {total_hits} ä»¶ã®è¦ç¢ºèªç®‡æ‰€ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ\n")
        print("=" * 70)

        for article_name, hits in flagged_articles:
            print(f"\nğŸ“„ {article_name}")
            for lineno, label, snippet in hits:
                print(f"  L{lineno:4d} [{label}]")
                print(f"         {snippet}")

        print("\n" + "=" * 70)
        print("\nç¢ºèªæ–¹æ³•:")
        print("  1. ä¸Šè¨˜ç®‡æ‰€ã‚’ WebSearch ã§æ¤œç´¢ã—ã¦æ•°å€¤ã®æ­£ç¢ºæ€§ã‚’ç¢ºèª")
        print("  2. ç¢ºèªã§ããªã‘ã‚Œã°ã€Œã€œã¨ã•ã‚Œã¦ã„ã¾ã™ã€ç­‰ã®å®šæ€§è¡¨ç¾ã«ç½®æ›")
        print("  3. ä¿®æ­£å¾Œã«å†åº¦ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ä»¶æ•°ãŒæ¸›ã£ãŸã‹ç¢ºèª")
        print("\nè¨±å®¹ã•ã‚Œã‚‹å¼•ç”¨ï¼ˆãã®ã¾ã¾ä½¿ç”¨å¯ï¼‰:")
        print("  - ã‚µãƒ¼ãƒ“ã‚¹è‡ªç¤¾å…¬é–‹ãƒ‡ãƒ¼ã‚¿ï¼ˆã€Œã€‡ã€‡ç¤¾å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã€ã¨æ˜ç¤ºï¼‰")
        print("  - æ”¿åºœçµ±è¨ˆï¼ˆå›½ç¨åºãƒ»æ–‡ç§‘çœãƒ»IIBCç­‰ï¼‰")
        print("  - è‘—åç†è«–ï¼ˆKrashenãƒ»Ebbinghausãƒ»Kuhlãƒ»Dweckç­‰ï¼‰â€”â€”%ãªã—")

    # â”€â”€ å¼•ç”¨ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    citation_entries = load_citation_keywords()
    if not citation_entries:
        return

    print("\n" + "=" * 70)
    print("ğŸ”— å¼•ç”¨ãƒªãƒ³ã‚¯ãƒã‚§ãƒƒã‚¯ï¼ˆadd_citations.py ã§è‡ªå‹•ä»˜ä¸å¯ï¼‰")
    print("=" * 70)

    link_total = 0
    link_flagged: list[tuple[str, list[tuple[str, str]]]] = []

    for name, html in html_files:
        hits2 = check_citation_links(html, citation_entries)
        if hits2:
            link_flagged.append((name, hits2))
            link_total += len(hits2)

    if not link_flagged:
        print("âœ… å…¨å¼•ç”¨ã‚½ãƒ¼ã‚¹ã«ãƒªãƒ³ã‚¯ãŒä»˜ä¸æ¸ˆã¿ã§ã™ã€‚")
    else:
        print(f"\nâš ï¸  {len(link_flagged)} è¨˜äº‹ã§ {link_total} ä»¶ã®å¼•ç”¨ãƒªãƒ³ã‚¯æœªä»˜ä¸\n")
        for article_name, hits2 in link_flagged:
            print(f"  ğŸ“„ {article_name}")
            for label, snippet in hits2:
                print(f"     [{label}] ã€Œ{snippet}ã€")
        print(f"\n  â†’ python3 add_citations.py ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")


if __name__ == "__main__":
    main()
